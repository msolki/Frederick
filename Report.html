<!DOCTYPE html>
  <html>
    <head>
      <title>Report</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/mohammadsolki/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.8.13/crossnote/dependencies/katex/katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><html><head><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body></body></html>
    </head>
    <body for="html-export" >
      <div class="crossnote markdown-preview  "  >
      
<h1 id="fredrick">Fredrick </h1>
<p><strong>Virtual AI Assistant for University of Naples Federico II</strong></p>
<div style="text-align: center;"><img src="https://upload.wikimedia.org/wikipedia/commons/5/5c/University_Federico_II_Logo.svg" alt="UNINA Logo" height="200px" width="200px"></div>
<p><em>Text Mining Course Project</em> <br> by <strong>Mohammad Solki</strong></p>
<p><em>A.Y. 2023/24</em></p>
<hr>
<p><strong>Table of Contents</strong></p>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#background-the-rise-of-ai-chatbots">Background: The Rise of AI Chatbots</a></li>
<li><a href="#frederick-the-virtual-assistant-for-unina-students">Frederick: The Virtual Assistant for UNINA Students</a></li>
<li><a href="#methodology">Methodology</a>
<ol>
<li><a href="#data-scraping">Data Scraping</a></li>
<li><a href="#rag-pipeline">RAG Pipeline</a></li>
<li><a href="#user-interface">User Interface</a></li>
</ol>
</li>
<li><a href="#capabilities-of-frederick-answering-student-questions">Capabilities of Frederick: Answering Student Questions</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
<li><a href="#appendix">Appendix</a></li>
<li><a href="#license">License</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#contact">Contact</a></li>
<li><a href="#project-status">Project Status</a></li>
<li><a href="#project-structure">Project Structure</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#authors-and-acknowledgement">Authors and Acknowledgement</a></li>
</ol>
<hr>
<blockquote>
<div style="text-align: center;">For Millennials, chatbots are a natural extension of texting. Millennials, having grown up with the Internet and responsive on-demand services, often cite phone calls as being overly intrusive, even presumptuous - thus their preference for texting. <br> <i>— Joseph Vincze (Library Hi Tech News)</i> </div>
</blockquote>
<style>
body {
  counter-reset: h2;
}
/*h1::before {*/
/*  counter-increment: h1;*/
/*  content: counter(h1) ". ";*/
/*}*/

h1 {
  counter-reset: h2;
}
h2 {
  counter-reset: h3;
}
h3 {
  counter-reset: h4;
}
h2::before {
  counter-increment: h2;
  content: counter(h2) ". ";
}
h3::before {
  counter-increment: h3;
  content: counter(h2) "." counter(h3) ". ";
}
h4::before {
  counter-increment: h4;
  content: counter(h4) ". ";
}

</style>
<h2 id="introduction">Introduction </h2>
<p>The increasing importance of Artificial Intelligence (AI) in various fields has led to the development of AI chatbots, which have revolutionized<br>
the way humans interact with machines. These conversational agents have become an essential component in many industries, including education.<br>
The University of Naples Federico II (UNINA) is no exception, as it has the potential to benefit form integrating AI-powered virtual assistants<br>
into its operations.</p>
<p>One such example is Frederick, a dedicated virtual assistant designed to provide students with accurate and helpful answers to their questions<br>
about UNINA's courses and programs. This innovative system utilizes a RAG pipeline, leveraging the power of Natural Language Processing (NLP) to<br>
understand and respond to user queries. The data for this chatbot was carefully scraped from the university website using two methods: a traditional<br>
approach using Selenium and a more advanced method utilizing Large Language Models (LLMs).</p>
<p>As Frederick's mission is to provide students with accurate and helpful answers, it is essential to develop its capabilities in understanding<br>
and responding to various types of questions. In this report, we will outline Frederick's role as a virtual assistant and explore the types of<br>
questions it can answer, including course details such as departments, schools, duration of courses, contact information, and more.</p>
<h2 id="background-the-rise-of-ai-chatbots">Background: The Rise of AI Chatbots </h2>
<p>The advent of Artificial Intelligence (AI) has revolutionized the way humans interact with machines, giving rise to a new generation of intelligent systems. Among these innovations are AI chatbots, also known as conversational agents or virtual assistants. These software applications use Natural Language Processing (NLP) and machine learning algorithms to understand and respond to human input in the form of text or voice commands.</p>
<p>In recent years, AI chatbots have become increasingly prevalent across various industries, including customer service, healthcare, finance, and more. Their ability to process vast amounts of data, provide rapid responses, and adapt to user behavior has made them an attractive solution for many organizations seeking to improve their communication with customers, employees, or students.</p>
<p>The increasing importance of AI chatbots in modern life is evident in the growing number of companies investing in these technologies. As a result, the development of sophisticated NLP capabilities, advanced machine learning algorithms, and improved user interfaces has become essential to stay ahead in this rapidly evolving landscape.</p>
<p>In this report, we will explore the role of AI chatbots in providing accurate and helpful answers to students' questions about UNINA's courses and programs, as embodied by Frederick, a dedicated virtual assistant designed to serve students at the University of Naples Federico II.</p>
<h2 id="frederick-the-virtual-assistant-for-unina-students">Frederick: The Virtual Assistant for UNINA Students </h2>
<p>As a dedicated virtual assistant, Frederick is designed to provide students with accurate and helpful answers to their questions about UNINA's courses and programs. This system utilizes a RAG pipeline, leveraging the power of Natural Language Processing (NLP) to understand and respond to user queries.</p>
<p>Frederick's primary mission is to serve as a valuable resource for current or prospective students of UNINA, providing them with essential information about course details, coordinator contact information, language of the course, duration, locations, and more. By doing so, Frederick aims to simplify the process of finding relevant information, reducing the time students spend searching for answers online.</p>
<p>To accomplish this mission, we carefully scraped data from the university website using two methods: a traditional approach using Selenium and a more advanced method utilizing Large Language Models (LLMs). This data will serve as the foundation for Frederick's knowledge base, enabling it to provide accurate and helpful responses to student queries.</p>
<p>In the following sections, we will delve into how Frederick is understanding and responding to various types of questions, showcasing its potential to improve the way students interact with UNINA.</p>
<h2 id="methodology">Methodology </h2>
<h3 id="data-scraping">Data Scraping </h3>
<blockquote>
<div style="text-align: center;">To the user, chatbots seem to be “intelligent” due to their informative skills. However, chatbots are only as intelligent as the underlying database. <br> <i>— Peter Gentsch (AI in Marketing, Sales and Service)</i></div>
</blockquote>
<p>To gather data for Frederick's knowledge base, we employed two methods: traditional scraping using Selenium and advanced scraping utilizing Large Language Models (LLMs). In this section, we will outline the methodology used for both approaches.</p>
<p><strong>Vanilla Scrape Methodology</strong></p>
<p>For the traditional scrape method, we utilized Selenium, a popular open-source tool for web scraping. However, we were unable to employ BeautifulSoup due to the UNINA website's use of JavaScript objects to store information. As such, we relied solely on Selenium for data extraction.</p>
<p>To scrape data, we followed these steps:</p>
<ol>
<li>Launched a Selenium WebDriver instance with Chrome browser and enabled headless mode to simulate a user's browsing experience.</li>
<li>Visited the UNINA website and used Selenium's <code>find_element</code> method to locate specific elements containing desired information using their XPath (e.g., course schedules, instructor contact details).</li>
<li>Extracted the data from these elements using Selenium's <code>text</code> property or by performing manual string manipulation.</li>
<li>Used WebDriverWait and expected_conditions modules to wait for dynamic content to load before scraping the data.</li>
<li>Stored the scraped data in a JSON file and a Pandas DataFrame for further processing.</li>
</ol>
<p><strong>LLM Scraping Methodology</strong></p>
<p>To leverage Large Language Models (LLMs) for advanced data scraping, we employed SmartScraperGraph, a powerful open-source tool that utilizes direct graph implementation to retrieve and extract relevant information from web pages.</p>
<p>We used the "gpt-3.5-turbo" model to process queries and generate coherent answers. Specifically, we:</p>
<ol>
<li>Loaded the UNINA website into the SmartScraperGraph scraper.</li>
<li>Provided specific query parameters to extract desired information (e.g., course details, coordinator, contact information).</li>
<li>The LLM model analyzed the webpage content and returned extracted data in a structured format.</li>
<li>Converted the retrieved data to JSON format.</li>
<li>Saved the resulting JSON file to an external file for future reference.</li>
</ol>
<p>By combining these two approaches, we were able to gather a comprehensive dataset for Frederick's knowledge base, enabling it to provide accurate and helpful responses to<br>
student queries.</p>
<h3 id="rag-pipeline">RAG Pipeline </h3>
<figure>
  <img src="https://i.ibb.co/jzwRBTN/Screenshot-2024-06-18-at-10-12-50.png" alt="RAG-Pipeline" style="width:100%">
  <figcaption style="text-align: center;">RAG Pipeline</figcaption>
</figure>
<p>In this section, we will outline the methodology used for the Retrieval-Augmented-Generation (RAG) pipeline, which enables Frederick to<br>
provide accurate and helpful responses to student queries.</p>
<h4 id="load-the-necessary-documents">Load the necessary documents </h4>
<p>In this step we imported the documents from the datasets prepared by the scraper. Including:</p>
<ul>
<li>List of university courses (json, csv)</li>
<li>Description and necessary information of each course (json, csv)</li>
</ul>
<p>Also:</p>
<ul>
<li>The system message for the chatbot (txt)</li>
<li>University international welcome-desk information (web)</li>
</ul>
<h4 id="efficient-document-processing-through-segmentation">Efficient Document Processing through Segmentation </h4>
<p>By breaking down large documents into smaller, more manageable chunks, we can streamline our processing efforts and achieve greater efficiency. This process, known as text segmentation or splitting, involves dividing a massive document into distinct, bite-sized pieces. By doing so, we can analyze and process the text in a more detailed and structured manner, enabling us to extract valuable insights and information from the data.</p>
<p>While segmenting documents may potentially disrupt the original context or meaning of the text, we can employ various techniques to mitigate this issue. For instance, we can utilize recursive splitting methods or incorporate techniques that preserve the original context and meaning of the text.</p>
<h4 id="vector-representation-using-embedding-models">Vector Representation Using Embedding Models </h4>
<p>We represent each document as a dense vector using pre-trained embedding models. When converting text into numerical representations, we rely on various embedding models to transform the text into dense vectors that capture its semantic meaning. Popular options include BERT, RoBERTa, GPT-2, among others. In this example, we utilize the MiniLM model, a smaller variant of the BERT architecture. By applying an embedding model, we can convert text into numerical vectors that encode the semantic essence of the text. These vectors can be employed for various natural language processing tasks, such as:</p>
<ul>
<li>Text classification: categorizing text based on its content</li>
<li>Clustering: grouping similar texts together</li>
<li>Similarity search: identifying the most relevant texts based on their semantic meaning</li>
</ul>
<p>The embedding model plays a crucial role in enabling these NLP applications by providing a compact and meaningful representation of the original text.</p>
<a href="https://www.elastic.co/what-is/vector-embedding">
<figure>
  <img src="https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt185ef72de6dc0e43/6466a9a1f21a3540facf75ac/vector-search-diagram-cropped-white-space.png" alt="Vector embeddings pipeline" style="width:100%">
  <figcaption style="text-align: center;">Vector embeddings</figcaption>
</figure>
</a>
<h4 id="efficient-embedding-retrieval-using-caching">Efficient Embedding Retrieval Using Caching </h4>
<p>When processing new documents, our approach first checks the cache for pre-computed embeddings. If these embeddings are not readily available, the method employs an underlying embedder to generate the embeddings and stores them in the cache for future reference.</p>
<p>By leveraging caching, we can:</p>
<ul>
<li>Avoid recalculating embeddings each time, which can be computationally expensive</li>
<li>Retrieve embeddings from memory, making our process faster and more efficient</li>
</ul>
<p>To optimize our embedding-based workflow, let's set up a cache to store pre-computed embeddings. This will enable us to quickly retrieve relevant<br>
information and reduce the computational overhead associated with re-calculating embeddings.</p>
<h4 id="vector-database-for-efficient-storage-and-retrieval">Vector Database for Efficient Storage and Retrieval </h4>
<p>Vector databases are designed to efficiently store and retrieve vectors, making them an essential component in applications such as information retrieval, recommendation systems, and clustering. These databases enable fast similarity searches by comparing query vectors with stored vectors, allowing us to quickly identify the most relevant matches.</p>
<p>In this notebook, we utilize the Chroma vector database, which is optimized for tasks involving similarity search and retrieval.</p>
<p><strong>Important Note</strong>: Due to the relatively small size of our documents, it's more efficient to pass entire documents to the vector database at once, rather than splitting them. This approach simplifies the process and minimizes potential errors.</p>
<p>After the initial vectorization step, we load pre-computed vectors from cache on subsequent calls, ensuring a seamless and optimized experience for future interactions with the vector database.</p>
<a href="https://www.graft.com/blog/top-open-source-vector-databases">
<figure>
  <img src="https://cdn.prod.website-files.com/640248e1fd70b63c09bd3d09/65ea3ef09d25332356575dee_chroma%20database%20architecture.svg" alt="Chroma is the open-source embedding database" style="width:100%">
  <figcaption style="text-align: center;">Chroma open-source vector embedding database</figcaption>
</figure>
</a>
<h4 id="utilizing-the-llm-with-groq-api">Utilizing the LLM with Groq API </h4>
<p>To leverage the power of language models, let's load the Llama model using the Groq API. This will enable us to tap into the capabilities of this<br>
pre-trained model without having to fine-tune it for our specific task. By utilizing the Groq API, we can streamline our workflow and optimize<br>
performance.</p>
<p>In particular, we plan to utilize the <code>nvidia/Llama3-ChatQA-1.5-8B</code> model from Hugging Face's model hub. This model has been specifically designed for<br>
chat-style question answering tasks and is expected to provide strong performance in this domain.</p>
<h4 id="evaluating-model-performance-on-unseen-questions">Evaluating Model Performance on Unseen Questions </h4>
<p>We put the model to the test by asking it questions and assessing its ability to provide accurate answers. Since we're working with an unseen dataset, this exercise will help us gauge the model's generalizability and ability to generalize beyond its training data.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Prompt</th>
<th style="text-align:left">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Who is the person in charge for autonomous vehicle engineering course in University of Naples Federico II?</td>
<td style="text-align:left">I've searched for the University of Naples Federico II's Autonomous Vehicle Engineering course, but I couldn't find a specific person in charge for the course. The university's website doesn't provide a single point of contact for the course. <br><br> However, I can suggest some alternatives to help you find the contact information: <br><br> 1. <strong>Department of Engineering</strong>: The Autonomous Vehicle Engineering course is likely part of the Department of Engineering at the University of Naples Federico II. You can visit the department's website and look for the contact information of the department's director or the person in charge of the course. <br><br> 2. <strong>Faculty of Engineering</strong>: Another option is to contact the Faculty of Engineering at the University of Naples Federico II. They may be able to provide you with the contact information of the person in charge of the Autonomous Vehicle Engineering course. <br><br> 3. <strong>Contact the University's International Office</strong>: The University of Naples Federico II has an International Office that handles international relations and collaborations. You can contact them and ask for assistance in finding the person in charge of the Autonomous Vehicle Engineering course. <br><br> Here are the contact details for the University of Naples Federico II: <br><li> University of Naples Federico II <br></li><li> Department of Engineering: <a href="http://ingegneria.unina.it">ingegneria.unina.it</a> <br> </li><li> Faculty of Engineering: <a href="http://ingegneria.unina.it/facolta">ingegneria.unina.it/facolta</a> <br></li><li> International Office: <a href="http://internationaloffice.unina.it">internationaloffice.unina.it</a> <br><br> Remember to be respectful and clear in your inquiry, and you may want to provide some background information about your interest in the Autonomous Vehicle Engineering course. Good luck!</li></td>
</tr>
</tbody>
</table>
<h4 id="contextual-compression-a-technique-for-enhanced-retrieval">Contextual Compression: A Technique for Enhanced Retrieval </h4>
<p><strong>NOTE:</strong> This method is not used in the final implementation.</p>
<p>In this implementation, we utilize contextual compression as a technique to condense the context surrounding a query into a more compact representation. This compressed context is then used to retrieve relevant information from our vector database.</p>
<p>By passing the compressed context to our retrieval model, we enable it to pinpoint the most pertinent documents or vectors with greater accuracy. This approach has the added benefit of reducing the search space, allowing us to focus on the most promising candidates and ultimately improving the overall retrieval process.</p>
<h4 id="crafting-effective-prompt-templates-for-llm-models">Crafting Effective Prompt Templates for LLM Models </h4>
<p><strong>NOTE:</strong> Below text was generated by Llama.</p>
<p>To elicit the best possible responses from our LLM models, we'll follow these guidelines to create high-quality prompt templates.</p>
<ul>
<li>Ensure that your prompts are concise and easy to understand, avoiding technical jargon and using natural language instead.</li>
<li>To provide additional context, ask the LLM to role-play as a character with a specific goal. For example, "You are a marketing executive who wants to invite major customers to a live event."</li>
<li>Refine your prompt templates by iterating on different approaches to achieve the same goal. Seek feedback from end-users and analyze how well your prompts generate desired responses.</li>
<li>Consistency is key: choose a writing style and stick to it, using consistent word choice, intensifiers, emojis, and punctuation to shape the LLM's<br>
response.</li>
<li>To help the LLM differentiate between context and instructions, create an "Instructions" section with triple quotes (""") in your prompt template.<br>
This will guide the model to focus on generating specific content rather than discussing the process.</li>
<li>Include direct instructions for the LLM to generate only the expected type of content. For example, if you want a draft email, specify that the<br>
response should strictly follow these guidelines.</li>
</ul>
<p>To get started, draw inspiration from the Example Prompt Template Library and customize templates to fit your unique needs. Study the language used<br>
in the examples, paying particular attention to writing style and phrasing. You can apply similar techniques to create effective prompt templates for<br>
your LLM models.</p>
<h4 id="crafting-contextualized-prompt-templates">Crafting Contextualized Prompt Templates </h4>
<p>When formulating prompt templates for our chatbot, we'll consider two key factors: the chat history and the latest user question. This context might<br>
be referenced in the chat history, but we need to create a standalone question that can be understood without it.</p>
<p>Our goal is not to answer the question, but rather to reformulate it if necessary, or return it as is if it's already suitable for our chatbot.</p>
<p>The suggested contextualized system prompt is:</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>contextualize_system_prompt <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"Given a chat history and the latest user question "</span>
    <span class="token string">"which might reference context in the chat history, "</span>
    <span class="token string">"formulate a standalone question which can be understood "</span>
    <span class="token string">"without the chat history. Do NOT answer the question, "</span>
    <span class="token string">"just reformulate it if needed and otherwise return it as is."</span>
<span class="token punctuation">)</span>

qa_system_prompt <span class="token operator">=</span>  SystemMessagePromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    contextualize_system_prompt
<span class="token punctuation">)</span>
</code></pre><p>The contextualized human prompt:</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>qa_human_prompt_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Question: {input}
Answer:"""</span>

qa_human_prompt <span class="token operator">=</span> HumanMessagePromptTemplate<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>PromptTemplate<span class="token punctuation">(</span>input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"question"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> template<span class="token operator">=</span>qa_human_prompt_template<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><p>And finally, the contextualized Q prompt will be:</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>contextualize_q_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_messages<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        qa_system_prompt<span class="token punctuation">,</span>
        MessagesPlaceholder<span class="token punctuation">(</span><span class="token string">"chat_history"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        qa_human_prompt<span class="token punctuation">,</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
</code></pre><h4 id="establishing-a-chain-of-thought">Establishing a Chain of Thought </h4>
<p>In this implementation we use the Chain-of-Thought Prompting technique to guide the LLM model through a series of prompts that build upon each other. This approach helps the model generate more coherent and contextually relevant responses by providing a structured framework for the conversation.</p>
<p>Contextual Retrieval with History Awareness, Sequential Processing and Augmentation such as Document Chain Creation and Retrieval Chain, Session History Storage, and Prompting Techniques such as Contextualization Prompts and QA Prompt Templates are used to enhance the retrieval process.</p>
<p><strong>Similarity-Based Search</strong></p>
<p>In the realm of data retrieval from vector databases, the "similarity" search type focuses on identifying documents or vectors that closely match a given query. This approach relies on mathematical metrics like cosine similarity or Euclidean distance to calculate the similarity between the query vector and those stored in the database. Documents with higher similarity scores are deemed more relevant and are returned as search results. The "similarity" search type is commonly employed for tasks such as document retrieval, recommendation systems, and information retrieval.</p>
<a href="https://arxiv.org/abs/2201.11903">
<figure>
  <img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&amp;w=1920&amp;q=75" alt="Standard Prompting vs. Chain-of-Thought Prompting" style="width:100%">
  <figcaption style="text-align: center;">Standard Prompting vs. Chain-of-Thought Prompting</figcaption>
</figure>
</a>
<h3 id="user-interface">User Interface </h3>
<p>Gradio is a user-friendly tool that enables you to create a web-based interface for your machine learning model. By defining the input and output types of your model, Gradio will automatically generate an interactive web interface that allows users to engage with your model. You can customize the interface by selecting from various input and output types, as well as styling options, to suit your specific needs.</p>
<p>Gradio also provides a shareable link to your interface, making it simple to share your model with others.</p>
<p>When evaluating the Learning Curve of Streamlit and Gradio, a notable distinction arises. Gradio excels in providing a more intuitive interface, making it ideal for users seeking a straightforward and quick development environment. On the other hand, while Streamlit offers advanced customization capabilities, it may require a slightly steeper learning curve for users aiming to harness its full potential.</p>
<blockquote>
<div style="text-align: center">"The contextual knowledge is embedded in one form — the capacity to rattle off linguistic knowledge — but is not embedded in another form" <br> <i>—Yann LeCun</i>
</div></blockquote>
<hr>
<p><strong>Resources</strong></p>
<ul>
<li>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., &amp; Kiela, D. (2021). <em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em> (<a href="http://arxiv.org/abs/2005.11401">arXiv:2005.11401</a>). arXiv.</p>
</li>
<li>
<p><a href="https://github.com/VinciGit00/Scrapegraph-ai">ScrapeGraphAI</a>: <em>A Python library for scraping leveraging large language models</em></p>
</li>
<li>
<p>Model <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><em>all-MiniLM-L6-v2</em></a>: Maps sentences &amp; paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.</p>
</li>
<li>
<p>Reimers, N., &amp; Gurevych, I. (2019). <em>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</em> (<a href="http://arxiv.org/abs/1908.10084">arXiv:1908.10084</a>). arXiv.</p>
</li>
<li>
<p><a href="https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#original-models">A comparison of sentence transformer pretrained_models</a></p>
</li>
<li>
<p>Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., &amp; Zhou, M. (2020). <em>MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</em> (<a href="http://arxiv.org/abs/2002.10957">arXiv:2002.10957</a>). arXiv.</p>
</li>
<li>
<p><a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/">Chroma</a>: an AI-native open-source vector database licensed under Apache 2.0 focused on developer productivity and happiness.</p>
</li>
<li>
<p><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Meta Llama3 on Groq</a> and <a href="https://ollama.com/library/llama3">Llama 3 from Ollama</a>: The most capable openly available LLM to date.</p>
</li>
<li>
<p><a href="https://python.langchain.com/v0.1/docs/use_cases/question_answering/">LangChain Use Cases - Q&amp;A with RAG</a></p>
</li>
<li>
<p><a href="https://github.com/sinadalvand/Galileo.git">Galileo: AI chatbot assistant for University of Padova</a></p>
</li>
<li>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., &amp; Zhou, D. (2023). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (<a href="http://arxiv.org/abs/2201.11903">arXiv:2201.11903</a>). arXiv.</p>
</li>
<li>
<p><a href="https://ai.plainenglish.io/langchain-in-chains-22-chain-of-thought-prompting-8b0dc4b01215">LangChain in Chains #22: Chain of Thought Prompting</a></p>
</li>
</ul>
<hr>
<div style="align-content: center"><img src="https://fuoridalcoro.altervista.org/wp-content/uploads/2023/11/UNINA-800-LOGO_segnaposto-eventi_620x300.jpg" alt="UNINA Logo 800 Years"></div>

      </div>
      
      
    </body>
    
    
    
    
    
  </html>